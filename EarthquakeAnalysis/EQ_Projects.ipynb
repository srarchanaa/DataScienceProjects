{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abh-0GUDAFUo"
      },
      "source": [
        "# 1. COMPLETE SCRIPT (Extraction → Cleaning → Enrichment → CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM8OxHjvAE5s",
        "outputId": "b81927a4-11af-466f-9dd1-c58f0c97275d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows : 20000\n",
            "Columns : 26\n",
            "             id                    time                 updated  latitude  \\\n",
            "0    us7000rs0p 2026-01-25 22:02:10.618 2026-01-26 01:55:14.040   23.3940   \n",
            "1    us7000rs0i 2026-01-25 20:29:01.866 2026-01-25 20:46:45.040   -8.6248   \n",
            "2  ak2026btoyfc 2026-01-25 19:51:55.287 2026-01-25 20:23:58.827   67.6520   \n",
            "3    us7000rs08 2026-01-25 19:24:40.117 2026-01-25 19:40:39.040  -31.4355   \n",
            "4    us7000rs01 2026-01-25 17:56:53.285 2026-01-25 18:18:13.040   37.5901   \n",
            "\n",
            "   longitude  depth_km  mag magType                          place    status  \\\n",
            "0   143.4290    10.000  5.0      mb  Volcano Islands, Japan region  reviewed   \n",
            "1   -74.6666   122.560  4.6      mb     16 km NNE of Honoria, Peru  reviewed   \n",
            "2  -166.6220     5.500  3.8      ml  77 km S of Point Hope, Alaska  reviewed   \n",
            "3   -71.5445    41.926  4.5     mwr    41 km WNW of Illapel, Chile  reviewed   \n",
            "4    21.4946    36.351  4.1      mb     4 km S of Epitálio, Greece  reviewed   \n",
            "\n",
            "   ...    gap  magError depthError  magNst  locationSource  magSource  \\\n",
            "0  ...   42.0      None       None    None            None       None   \n",
            "1  ...  100.0      None       None    None            None       None   \n",
            "2  ...  188.0      None       None    None            None       None   \n",
            "3  ...  145.0      None       None    None            None       None   \n",
            "4  ...  128.0      None       None    None            None       None   \n",
            "\n",
            "                               types                        ids  sources  \\\n",
            "0                ,origin,phase-data,               ,us7000rs0p,     ,us,   \n",
            "1                ,origin,phase-data,               ,us7000rs0i,     ,us,   \n",
            "2       ,origin,phase-data,shakemap,  ,us7000rs0e,ak2026btoyfc,  ,us,ak,   \n",
            "3  ,moment-tensor,origin,phase-data,               ,us7000rs08,     ,us,   \n",
            "4                ,origin,phase-data,               ,us7000rs01,     ,us,   \n",
            "\n",
            "         type  \n",
            "0  earthquake  \n",
            "1  earthquake  \n",
            "2  earthquake  \n",
            "3  earthquake  \n",
            "4  earthquake  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "# -----------------------------\n",
        "# Date range: last 5 years\n",
        "# -----------------------------\n",
        "end_date = datetime.now(timezone.utc)\n",
        "start_date = end_date - timedelta(days=5 * 365)\n",
        "\n",
        "# -----------------------------\n",
        "# USGS API endpoint\n",
        "# -----------------------------\n",
        "url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
        "\n",
        "params = {\n",
        "    \"format\": \"geojson\",\n",
        "    \"starttime\": start_date.strftime(\"%Y-%m-%d\"),\n",
        "    \"endtime\": end_date.strftime(\"%Y-%m-%d\"),\n",
        "    \"minmagnitude\": 3,\n",
        "    \"limit\": 20000\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "response.raise_for_status()\n",
        "data = response.json()\n",
        "\n",
        "# -----------------------------\n",
        "# Extract records\n",
        "# -----------------------------\n",
        "records = []\n",
        "\n",
        "for feature in data[\"features\"]:\n",
        "    props = feature[\"properties\"]\n",
        "    coords = feature[\"geometry\"][\"coordinates\"]\n",
        "\n",
        "    records.append({\n",
        "        \"id\": feature[\"id\"],\n",
        "\n",
        "        \"time\": pd.to_datetime(props.get(\"time\"), unit=\"ms\", errors=\"coerce\"),\n",
        "        \"updated\": pd.to_datetime(props.get(\"updated\"), unit=\"ms\", errors=\"coerce\"),\n",
        "\n",
        "        \"latitude\": coords[1],\n",
        "        \"longitude\": coords[0],\n",
        "        \"depth_km\": coords[2],\n",
        "\n",
        "        \"mag\": props.get(\"mag\"),\n",
        "        \"magType\": props.get(\"magType\"),\n",
        "        \"place\": props.get(\"place\"),\n",
        "        \"status\": props.get(\"status\"),\n",
        "        \"tsunami\": props.get(\"tsunami\"),\n",
        "        \"sig\": props.get(\"sig\"),\n",
        "\n",
        "        \"net\": props.get(\"net\"),\n",
        "        \"nst\": props.get(\"nst\"),\n",
        "        \"dmin\": props.get(\"dmin\"),\n",
        "        \"rms\": props.get(\"rms\"),\n",
        "        \"gap\": props.get(\"gap\"),\n",
        "\n",
        "        \"magError\": props.get(\"magError\"),\n",
        "        \"depthError\": props.get(\"depthError\"),\n",
        "        \"magNst\": props.get(\"magNst\"),\n",
        "\n",
        "        \"locationSource\": props.get(\"locationSource\"),\n",
        "        \"magSource\": props.get(\"magSource\"),\n",
        "\n",
        "        \"types\": props.get(\"types\"),\n",
        "        \"ids\": props.get(\"ids\"),\n",
        "        \"sources\": props.get(\"sources\"),\n",
        "        \"type\": props.get(\"type\")\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Create DataFrame\n",
        "# -----------------------------\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "print(\"Rows :\", df.shape[0])\n",
        "print(\"Columns :\", df.shape[1])\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhlNBcv6_T3n"
      },
      "source": [
        "# 2. Clean Text Fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TmbPPNKH_TNC"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Extract country from place\n",
        "# -----------------------------\n",
        "def extract_country(place):\n",
        "    if pd.isna(place):\n",
        "        return None\n",
        "    parts = place.split(\",\")\n",
        "    return parts[-1].strip() if len(parts) > 1 else None\n",
        "\n",
        "df[\"country\"] = df[\"place\"].apply(extract_country)\n",
        "\n",
        "# -----------------------------\n",
        "# Normalize alert field (if exists)\n",
        "# -----------------------------\n",
        "if \"alert\" in df.columns:\n",
        "    df[\"alert\"] = df[\"alert\"].astype(str).str.lower().replace(\"nan\", None)\n",
        "\n",
        "# -----------------------------\n",
        "# Clean string fields\n",
        "# -----------------------------\n",
        "text_cols = [\n",
        "    \"magType\", \"status\", \"type\", \"net\",\n",
        "    \"sources\", \"types\", \"locationSource\", \"magSource\"\n",
        "]\n",
        "\n",
        "for col in text_cols:\n",
        "    df[col] = (\n",
        "        df[col]\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .str.lower()\n",
        "        .replace(\"nan\", None)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbgxPFllAhFr"
      },
      "source": [
        "# 3. Clean Numeric Fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5p5zRzHzAmRO"
      },
      "outputs": [],
      "source": [
        "numeric_cols = [\n",
        "    \"mag\", \"depth_km\", \"nst\", \"dmin\", \"rms\",\n",
        "    \"gap\", \"magError\", \"depthError\", \"magNst\", \"sig\"\n",
        "]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Fill missing numeric values\n",
        "df[numeric_cols] = df[numeric_cols].fillna(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccTsH5qHAueG"
      },
      "source": [
        "# 4. Add derived columns (Date Components, Depth Classification, Magnitude Impact Classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FcGfCgMoA0fI"
      },
      "outputs": [],
      "source": [
        "df[\"year\"] = df[\"time\"].dt.year\n",
        "df[\"month\"] = df[\"time\"].dt.month\n",
        "df[\"day\"] = df[\"time\"].dt.day\n",
        "df[\"day_of_week\"] = df[\"time\"].dt.day_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fXhaI7TeA24N"
      },
      "outputs": [],
      "source": [
        "df[\"depth_category\"] = np.where(\n",
        "    df[\"depth_km\"] <= 70, \"shallow\",\n",
        "    np.where(df[\"depth_km\"] <= 300, \"intermediate\", \"deep\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BvtTKwMqBSUO"
      },
      "outputs": [],
      "source": [
        "def magnitude_flag(mag):\n",
        "    if mag >= 7.0:\n",
        "        return \"destructive\"\n",
        "    elif mag >= 5.0:\n",
        "        return \"strong\"\n",
        "    elif mag >= 3.0:\n",
        "        return \"moderate\"\n",
        "    else:\n",
        "        return \"weak\"\n",
        "\n",
        "df[\"impact_level\"] = df[\"mag\"].apply(magnitude_flag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmZGSe1lBb_E",
        "outputId": "d497656a-319b-4bbe-e8a1-ca09fce7adcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV exported successfully\n"
          ]
        }
      ],
      "source": [
        "# df.to_csv(\n",
        "#     \"earthquakes_clean_last_5_years.csv\",\n",
        "#     index=False,\n",
        "#     encoding=\"utf-8-sig\"\n",
        "# )\n",
        "\n",
        "# print(\"CSV exported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1cRJUtejcrf_"
      },
      "outputs": [],
      "source": [
        "# df.to_csv(\n",
        "#    \"earthquakes_workbench.csv\",\n",
        "#    index=False,\n",
        "#    encoding=\"utf-8-sig\"  # IMPORTANT\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GbWj73Gt4h8X"
      },
      "outputs": [],
      "source": [
        "def to_ascii(val):\n",
        "    if isinstance(val, str):\n",
        "        return val.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n",
        "    return val\n",
        "\n",
        "df_ascii = df.map(to_ascii)\n",
        "\n",
        "df_ascii.to_csv(\n",
        "    \"earthquakes_clean_last_5_years1.csv\",\n",
        "    index=False,\n",
        "    encoding=\"ascii\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"earthquakes_clean_last_5_years1.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
